{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "O__cBBmP8CjW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5uy_McU721G"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "from copy import deepcopy\n",
        "import os\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "config"
      ],
      "metadata": {
        "id": "9Hx92Kyn8IWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    DATA_DIR = \"./data\"\n",
        "    IMAGE_SIZE = 28\n",
        "    IMAGE_CHANNELS = 1\n",
        "    NUM_CLASSES = 10\n",
        "    BATCH_SIZE = 128\n",
        "    NUM_WORKERS = 2\n",
        "\n",
        "    N_STEPS = 1000\n",
        "    LEARNING_RATE = 1e-4\n",
        "    N_EPOCHS = 100\n",
        "    SAVE_EVERY_N_EPOCHS = 10\n",
        "    EMA_DECAY = 0.999\n",
        "    KL_WEIGHT = 1e-4\n",
        "\n",
        "    LATENT_CHANNELS = 4\n",
        "    LATENT_SIZE = 7\n",
        "    VAE_EPOCHS = 35\n",
        "    VAE_PATH = \"fashion_vae.pt\"\n",
        "\n",
        "    LATENT_MODEL_PATH = \"latent_model.pt\"\n",
        "    LATENT_EMA_PATH = \"latent_model.ema.pt\"\n",
        "\n",
        "    N_SAMPLES = 80\n",
        "    GUIDANCE_WEIGHT = 0.0\n",
        "\n",
        "    BASE_CHANNELS = 64\n",
        "    CHANNEL_MULT = [1, 2, 2]\n",
        "    NUM_RES_BLOCKS = 2\n",
        "    TIME_EMBED_DIM = BASE_CHANNELS*4\n",
        "    CONTEXT_DIM = TIME_EMBED_DIM\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "-lBwICBl8F7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "l82q402M8RLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(train: bool = True):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "    dataset = datasets.FashionMNIST(\n",
        "        root=config.DATA_DIR,\n",
        "        train=train,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config.BATCH_SIZE,\n",
        "        shuffle=train,\n",
        "        num_workers=config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )"
      ],
      "metadata": {
        "id": "D7U_ER3k8F9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_timestep_embedding(timesteps, dim):\n",
        "    half_dim = dim // 2\n",
        "    exponent = torch.exp(-math.log(10000) * torch.arange(half_dim, device=timesteps.device) / half_dim)\n",
        "    timesteps = timesteps.float().unsqueeze(-1)\n",
        "    emb = timesteps * exponent.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "    return emb.to(dtype=torch.float32)\n",
        "\n",
        "def save_checkpoint(model, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "def load_checkpoint(model, path, device=\"cpu\"):\n",
        "    if os.path.exists(path):\n",
        "        model.load_state_dict(torch.load(path, map_location=device))\n",
        "        print(f\"Checkpoint loaded from {path}\")\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "yhxD0HNe8F_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE"
      ],
      "metadata": {
        "id": "ukwKncuh8Z63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, channels, reduction=8):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_channels=config.LATENT_CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(config.IMAGE_CHANNELS, 32, 3, padding=1),\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.SiLU(),\n",
        "            ChannelAttention(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.SiLU(),\n",
        "            ChannelAttention(128)\n",
        "        )\n",
        "\n",
        "        self.to_mu = nn.Conv2d(128, latent_channels, 1)\n",
        "        self.to_logvar = nn.Conv2d(128, latent_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.net(x)\n",
        "        return self.to_mu(h), self.to_logvar(h)\n",
        "\n",
        "def reparameterize(mu, logvar):\n",
        "    std = torch.exp(0.5 * logvar)\n",
        "    eps = torch.randn_like(std)\n",
        "    return mu + eps * std\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_channels=config.LATENT_CHANNELS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(latent_channels, 128, 3, padding=1),\n",
        "            nn.GroupNorm(8, 128),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.SiLU(),\n",
        "            ChannelAttention(64),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.SiLU(),\n",
        "\n",
        "            nn.Conv2d(32, config.IMAGE_CHANNELS, 3, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, latent_channels=config.LATENT_CHANNELS):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(latent_channels)\n",
        "        self.decoder = Decoder(latent_channels)\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encoder(x)\n",
        "        z = reparameterize(mu, logvar)\n",
        "        recon = self.decoder(z)\n",
        "        return recon, mu, logvar, z"
      ],
      "metadata": {
        "id": "5PYdSaJK8GCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNet"
      ],
      "metadata": {
        "id": "cupCnqGs8guY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim, num_heads=4):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(query_dim, query_dim)\n",
        "        self.k_proj = nn.Linear(context_dim, query_dim)\n",
        "        self.v_proj = nn.Linear(context_dim, query_dim)\n",
        "        self.attn = nn.MultiheadAttention(query_dim, num_heads, batch_first=True)\n",
        "        self.proj_out = nn.Linear(query_dim, query_dim)\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        q = self.q_proj(x)\n",
        "        k = self.k_proj(context)\n",
        "        v = self.v_proj(context)\n",
        "        attn_output, _ = self.attn(q, k, v)\n",
        "        return x + self.proj_out(attn_output)\n",
        "\n",
        "class LatentResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, time_emb_dim, context_dim):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, out_ch))\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.shortcut = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        self.norm_ca = nn.GroupNorm(8, out_ch)\n",
        "        self.ca = CrossAttention(out_ch, context_dim)\n",
        "\n",
        "    def forward(self, x, t_emb, context):\n",
        "        b, _, h, w = x.shape\n",
        "        out_ch = self.conv1.out_channels\n",
        "\n",
        "        h_temp = F.group_norm(x, min(8, x.shape[1]))\n",
        "        h_temp = self.conv1(F.silu(h_temp))\n",
        "\n",
        "        h_temp = h_temp + self.time_mlp(t_emb)[:, :, None, None]\n",
        "\n",
        "        h_ca = F.silu(self.norm_ca(h_temp))\n",
        "        new_hw = int(h) * int(w)\n",
        "        h_ca = h_ca.permute(0, 2, 3, 1).reshape(b, new_hw, out_ch)\n",
        "        h_ca = self.ca(h_ca, context)\n",
        "        h_temp = h_temp + h_ca.transpose(1, 2).reshape(b, out_ch, h, w)\n",
        "\n",
        "        h_temp = self.conv2(F.silu(F.group_norm(h_temp, min(8, h_temp.shape[1]))))\n",
        "\n",
        "        return h_temp + self.shortcut(x)\n",
        "\n",
        "class LatentEpsModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        time_dim = config.TIME_EMBED_DIM\n",
        "        context_dim = config.CONTEXT_DIM\n",
        "\n",
        "        self.time_mlp = nn.Sequential(nn.Linear(time_dim, time_dim), nn.SiLU(), nn.Linear(time_dim, time_dim))\n",
        "        self.class_emb = nn.Embedding(config.NUM_CLASSES, context_dim)\n",
        "        self.conv_in = nn.Conv2d(config.LATENT_CHANNELS, config.BASE_CHANNELS, 3, padding=1)\n",
        "\n",
        "        self.downs = nn.ModuleList()\n",
        "        ch = config.BASE_CHANNELS\n",
        "        for mult in config.CHANNEL_MULT:\n",
        "            out = config.BASE_CHANNELS * mult\n",
        "            blocks = nn.ModuleList()\n",
        "            blocks.append(LatentResBlock(ch, out, time_dim, context_dim))\n",
        "            for _ in range(config.NUM_RES_BLOCKS - 1):\n",
        "                blocks.append(LatentResBlock(out, out, time_dim, context_dim))\n",
        "\n",
        "            downsample = nn.Conv2d(out, out, 3, 1, 1)\n",
        "            self.downs.append(nn.ModuleDict({\"blocks\": blocks, \"down\": downsample}))\n",
        "            ch = out\n",
        "\n",
        "        self.bot1 = LatentResBlock(ch, ch, time_dim, context_dim)\n",
        "        self.bot2 = LatentResBlock(ch, ch, time_dim, context_dim)\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for mult in reversed(config.CHANNEL_MULT):\n",
        "            skip_ch = config.BASE_CHANNELS * mult\n",
        "            out = skip_ch\n",
        "            in_ch_res = out + skip_ch\n",
        "\n",
        "            blocks = nn.ModuleList()\n",
        "            blocks.append(LatentResBlock(in_ch_res, out, time_dim, context_dim))\n",
        "            for _ in range(config.NUM_RES_BLOCKS - 1):\n",
        "                blocks.append(LatentResBlock(out, out, time_dim, context_dim))\n",
        "\n",
        "            upsample = nn.ConvTranspose2d(ch, out, 3, 1, 1)\n",
        "            self.ups.append(nn.ModuleDict({\"blocks\": blocks, \"up\": upsample}))\n",
        "            ch = out\n",
        "\n",
        "        self.conv_out = nn.Conv2d(ch, config.LATENT_CHANNELS, 3, padding=1)\n",
        "\n",
        "    def forward(self, x, t, y):\n",
        "        t_emb = self.time_mlp(get_timestep_embedding(t, config.TIME_EMBED_DIM))\n",
        "        context = self.class_emb(y).unsqueeze(1)\n",
        "\n",
        "        hs = []\n",
        "        h = self.conv_in(x)\n",
        "        for module in self.downs:\n",
        "            for block in module[\"blocks\"]:\n",
        "                h = block(h, t_emb, context)\n",
        "            hs.append(h)\n",
        "            h = module[\"down\"](h)\n",
        "\n",
        "        h = self.bot1(h, t_emb, context)\n",
        "        h = self.bot2(h, t_emb, context)\n",
        "\n",
        "        for module in self.ups:\n",
        "            skip = hs.pop()\n",
        "            h = module[\"up\"](h)\n",
        "\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            for block in module[\"blocks\"]:\n",
        "                h = block(h, t_emb, context)\n",
        "\n",
        "        return self.conv_out(h)"
      ],
      "metadata": {
        "id": "E-StYNOE8GEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sheduler"
      ],
      "metadata": {
        "id": "buSVRJqV8lZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDenoiseDiffusion():\n",
        "    def __init__(self, eps_model, n_steps=config.N_STEPS, device=None):\n",
        "        super().__init__()\n",
        "        self.eps_model = eps_model\n",
        "        self.device = device if device is not None else torch.device(\"cpu\")\n",
        "\n",
        "        self.beta = torch.linspace(0.0001, 0.02, n_steps).to(self.device)\n",
        "        self.alpha = 1 - self.beta\n",
        "        self.alpha_bar = torch.cumprod(self.alpha, dim=0)\n",
        "        self.n_steps = n_steps\n",
        "        self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "\n",
        "        self.alphas_cumprod_prev = F.pad(self.alpha_bar[:-1], (1, 0), value=1.0)\n",
        "        self.post_variance = self.beta * (1. - self.alphas_cumprod_prev) / (1. - self.alpha_bar)\n",
        "\n",
        "    def q_sample(self, x0, t, eps=None):\n",
        "        if eps is None:\n",
        "            eps = torch.randn_like(x0)\n",
        "        a_bar = self.sqrt_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "        one_minus = self.sqrt_one_minus_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "        return a_bar * x0 + one_minus * eps\n",
        "\n",
        "    def p_sample(self, xt, t, c=None):\n",
        "        if not isinstance(t, torch.Tensor):\n",
        "            t = torch.tensor([t] * xt.shape[0], device=xt.device, dtype=torch.long)\n",
        "\n",
        "        eps_theta = self.eps_model(xt, t, c)\n",
        "\n",
        "        alpha_t = self.alpha[t].reshape(-1, 1, 1, 1)\n",
        "        alpha_bar_t = self.alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "        alpha_bar_t_prev = self.alphas_cumprod_prev[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "        x0_pred = (xt - self.sqrt_one_minus_alpha_bar[t].reshape(-1, 1, 1, 1) * eps_theta) / self.sqrt_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "        x0_pred = torch.clamp(x0_pred, -1., 1.)\n",
        "\n",
        "        mean = (alpha_bar_t_prev.sqrt() * self.beta[t].reshape(-1, 1, 1, 1) / (1. - alpha_bar_t)) * x0_pred + \\\n",
        "               (alpha_t.sqrt() * (1. - alpha_bar_t_prev) / (1. - alpha_bar_t)) * xt\n",
        "\n",
        "        variance = self.post_variance[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "        if t[0] > 0:\n",
        "            noise = torch.randn_like(xt)\n",
        "            return mean + torch.sqrt(variance) * noise\n",
        "        else:\n",
        "            return mean\n",
        "\n",
        "    def sample(self, shape, device, c=None):\n",
        "        x = torch.randn(shape, device=device)\n",
        "        for t in tqdm(reversed(range(self.n_steps)), desc=\"Sampling\"):\n",
        "            x = self.p_sample(x, t, c)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x0, labels=None):\n",
        "        batch_size = x0.shape[0]\n",
        "        t = torch.randint(0, self.n_steps, (batch_size,), device=x0.device, dtype=torch.long)\n",
        "        eps = torch.randn_like(x0)\n",
        "        xt = self.q_sample(x0, t, eps)\n",
        "        eps_theta = self.eps_model(xt, t, labels)\n",
        "        return F.mse_loss(eps, eps_theta)"
      ],
      "metadata": {
        "id": "nB6TwTf68GGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Sampling"
      ],
      "metadata": {
        "id": "R2URUqwL8rER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_vae(device):\n",
        "    vae = VAE().to(device)\n",
        "\n",
        "    for p in vae.parameters():\n",
        "        p.requires_grad_(True)\n",
        "\n",
        "    vae_optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "    dataloader = get_dataloader(train=True)\n",
        "\n",
        "    print(\"--- Starting Attention-Enhanced VAE Pre-training (Fashion MNIST) ---\")\n",
        "    for epoch in range(config.VAE_EPOCHS):\n",
        "        pbar = tqdm(dataloader, desc=f\"VAE Epoch {epoch+1}/{config.VAE_EPOCHS}\")\n",
        "        for imgs, _ in pbar:\n",
        "            imgs = imgs.to(device)\n",
        "            vae_optimizer.zero_grad()\n",
        "\n",
        "            recon, mu, logvar, _ = vae(imgs)\n",
        "\n",
        "            recon_loss = F.mse_loss(recon, imgs, reduction='mean')\n",
        "            kl_div = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "            loss = recon_loss + config.KL_WEIGHT * kl_div\n",
        "\n",
        "            loss.backward()\n",
        "            vae_optimizer.step()\n",
        "            pbar.set_postfix(recon=f\"{recon_loss.item():.4f}\", kl=f\"{kl_div.item():.4f}\")\n",
        "\n",
        "    save_checkpoint(vae, config.VAE_PATH)\n",
        "    print(f\"Fashion VAE weights saved to {config.VAE_PATH}. VAE training complete.\")\n",
        "    return vae\n",
        "\n",
        "def sample_vae(device, vae):\n",
        "    if not load_checkpoint(vae, config.VAE_PATH, device=device):\n",
        "        print(\"ERROR: VAE weights not found. Cannot sample VAE.\")\n",
        "        return\n",
        "    vae.eval()\n",
        "\n",
        "    dataloader = get_dataloader(train=False)\n",
        "    imgs, _ = next(iter(dataloader))\n",
        "    imgs = imgs[:10].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        recon, _, _, _ = vae(imgs)\n",
        "\n",
        "    combined = torch.cat([imgs, recon], dim=0)\n",
        "\n",
        "    out_path = \"vae_reconstruction_check.png\"\n",
        "    save_image(combined, out_path, nrow=10)\n",
        "    print(f\"VAE Reconstruction check saved to {out_path}.\")\n",
        "\n",
        "def train_latent_ddpm(device, vae):\n",
        "    if not load_checkpoint(vae, config.VAE_PATH, device=device):\n",
        "        print(\"ERROR: VAE weights not found. Please run 'train_vae' first!\")\n",
        "        return\n",
        "\n",
        "    for p in vae.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    vae.eval()\n",
        "    print(\"Attention-Enhanced VAE is loaded and frozen.\")\n",
        "\n",
        "    model = LatentEpsModel().to(device)\n",
        "    ema_model = deepcopy(model)\n",
        "    for p in ema_model.parameters():\n",
        "        p.requires_grad_(False)\n",
        "\n",
        "    load_checkpoint(model, config.LATENT_MODEL_PATH, device=device)\n",
        "\n",
        "    sched = ConditionalDenoiseDiffusion(model, device=device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    dataloader = get_dataloader(train=True)\n",
        "\n",
        "    print(\"--- Starting Fashion CLDM Training ---\")\n",
        "    for epoch in range(config.N_EPOCHS):\n",
        "        model.train()\n",
        "        pbar = tqdm(dataloader, desc=f\"CLDM Epoch {epoch+1}/{config.N_EPOCHS}\")\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for imgs, labels in pbar:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                mu, log_var = vae.encode(imgs)\n",
        "                z0 = mu + torch.randn_like(mu) * torch.exp(0.5 * log_var)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = sched.loss(z0, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for ema_p, p in zip(ema_model.parameters(), model.parameters()):\n",
        "                    ema_p.mul_(config.EMA_DECAY).add_(p * (1 - config.EMA_DECAY))\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        print(f\"\\n--- Epoch {epoch+1} finished. Avg Loss: {total_loss / len(dataloader):.4f} ---\")\n",
        "\n",
        "        if (epoch+1) % config.SAVE_EVERY_N_EPOCHS == 0 or (epoch+1) == config.N_EPOCHS:\n",
        "            save_checkpoint(model, config.LATENT_MODEL_PATH)\n",
        "            save_checkpoint(ema_model, config.LATENT_EMA_PATH)\n",
        "            print(f\"Fashion CLDM Checkpoint saved at epoch {epoch+1}\")\n",
        "\n",
        "            sample_latent_images(device, vae, ema_model)\n",
        "    return model, ema_model\n",
        "\n",
        "def sample_latent_images(device, vae, model=None):\n",
        "\n",
        "    if model is None:\n",
        "        model = LatentEpsModel().to(device)\n",
        "        if not load_checkpoint(model, config.LATENT_EMA_PATH, device=device):\n",
        "            print(\"ERROR: EMA Latent model not found. Cannot sample.\")\n",
        "            return\n",
        "\n",
        "    if not load_checkpoint(vae, config.VAE_PATH, device=device):\n",
        "        print(\"ERROR: VAE weights not found. Cannot sample.\")\n",
        "        return\n",
        "\n",
        "    for p in vae.parameters():\n",
        "        p.requires_grad_(False)\n",
        "    vae.eval()\n",
        "    model.eval()\n",
        "\n",
        "    sched = ConditionalDenoiseDiffusion(model, device=device)\n",
        "\n",
        "    n_per_class = config.N_SAMPLES // config.NUM_CLASSES\n",
        "\n",
        "    target_labels = torch.arange(config.NUM_CLASSES, device=device).repeat_interleave(n_per_class)\n",
        "\n",
        "    latent_shape = (config.N_SAMPLES, config.LATENT_CHANNELS, config.LATENT_SIZE, config.LATENT_SIZE)\n",
        "    print(f\"Generating {config.N_SAMPLES} samples in latent space {latent_shape}...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z_samples = sched.sample(\n",
        "            shape=latent_shape,\n",
        "            device=device,\n",
        "            c=target_labels\n",
        "        )\n",
        "\n",
        "        x_samples = vae.decode(z_samples).clamp(-1, 1)\n",
        "\n",
        "    x_samples = (x_samples + 1) * 0.5\n",
        "    out_path = \"fashion_latent_samples.png\"\n",
        "    save_image(x_samples, out_path, nrow=n_per_class)\n",
        "    print(f\"Generated samples saved to {out_path}.\")"
      ],
      "metadata": {
        "id": "68A1RSd48GIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0hDPDuF8GKd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}