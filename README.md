# FashionMNIST Stable Diffusion: Generative Modeling for Apparel

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Python](https://img.shields.io/badge/Python-3.x-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

## üåü Project Overview

This project implements a **Stable Diffusion** model specifically adapted for the **FashionMNIST** dataset. The primary objective is to demonstrate the application of latent diffusion models in generating high-quality, novel images of apparel items. By tailoring the diffusion architecture to the simpler, grayscale FashionMNIST domain, this work provides a clear, educational, and reproducible example of how these powerful generative models can be applied to structured image datasets.

The implementation is built from scratch using the PyTorch framework, focusing on the core components of a latent diffusion model: the **Variational Autoencoder (VAE)** and the **UNet denoiser**.

## üñºÔ∏è Results and Sample Generation

Below is a sample of images generated by the trained model, showcasing its ability to synthesize realistic and diverse apparel items from the FashionMNIST classes.

<div align="center">
    <!-- Placeholder for a generated image grid -->
    <img src="assets/generated_samples.png" alt="Sample images generated by the Stable Diffusion model on FashionMNIST" width="600"/>
    <p><i>(Note: A grid of sample images generated by the model should be placed here for a professional presentation.)</i></p>
</div>

## ‚ú® Key Features

*   **Latent Diffusion Architecture:** Full implementation of a latent diffusion model, including a custom VAE for compressing images into a lower-dimensional latent space and a UNet for the iterative denoising process.
*   **FashionMNIST Integration:** Specifically configured to train on the 28x28 grayscale images of the FashionMNIST dataset, demonstrating effective generative modeling on a structured, real-world image classification benchmark.
*   **PyTorch Implementation:** All models and training loops are implemented using the PyTorch framework, making the code highly flexible and suitable for further research and experimentation.
*   **Educational Notebooks:** Comprehensive Jupyter Notebooks guide the user through the entire process, from data loading and model definition to training and image generation.

## ‚öôÔ∏è Technical Deep Dive

The project's architecture is composed of three main components: the VAE, the UNet, and the Diffusion Process.

### Model Architecture Summary

| Component | Role | Key Features |
| :--- | :--- | :--- |
| **Variational Autoencoder (VAE)** | Encodes images into a latent space and decodes latent vectors back to images. | Custom Encoder and Decoder modules, ChannelAttention, GroupNorm, SiLU activation. |
| **UNet** | The core denoising network. Predicts noise added to the latent vector at each diffusion step. | Includes CrossAttention and LatentResBlock components for robust feature extraction. |
| **Diffusion Process** | The iterative process of adding and removing noise to generate new samples. | Implements a 1000-step process with custom timestep embeddings. |

### Training Configuration

The following parameters were used for training the model, as detailed in the configuration class within the notebooks:

| Parameter | Value | Description |
| :--- | :--- | :--- |
| **Dataset** | FashionMNIST | 28x28 grayscale images of 10 apparel classes. |
| **Image Size** | 28x28 | Standard resolution for the FashionMNIST dataset. |
| **Latent Size** | 7x7 | The spatial dimension of the compressed latent representation. |
| **Steps** | 1000 | The number of forward and reverse diffusion steps. |
| **Epochs** | 100 | Total number of training epochs. |
| **Batch Size** | 128 | Number of samples processed per training iteration. |
| **Learning Rate** | 1e-4 | The rate at which model weights are updated during training. |

## üìÅ Repository Structure

The repository is organized to separate the model implementation, training scripts, and documentation:

```
.
‚îú‚îÄ‚îÄ assets/                     # Directory for images used in the README (e.g., generated_samples.png).
‚îú‚îÄ‚îÄ model/                      # Saved model checkpoints (.pt files).
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter Notebooks for training and application.
‚îÇ   ‚îú‚îÄ‚îÄ fashionMNIST_stableDiffusion.ipynb      # Main training and model definition notebook.
‚îÇ   ‚îî‚îÄ‚îÄ fashionMNIST_stableDiffusionApplication.ipynb # Notebook for image generation and application.
‚îú‚îÄ‚îÄ presentation/               # Project presentation or report materials (e.g., stable_diffusion.pdf).
‚îú‚îÄ‚îÄ .gitignore                  # Files and directories to be ignored by Git.
‚îú‚îÄ‚îÄ LICENSE                     # Project license (MIT).
‚îú‚îÄ‚îÄ requirements.txt            # List of Python dependencies.
‚îî‚îÄ‚îÄ README.md                   # Project documentation (this file).
```

## üöÄ Getting Started

### Prerequisites

*   Python 3.x
*   A machine with a modern GPU (recommended for faster training).

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/SeifEldenOsama/fashionMNIST_StableDiffusion.git
    cd fashionMNIST_StableDiffusion
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

### Usage

The primary way to interact with this project is through the Jupyter Notebooks in the `notebooks/` directory.

1.  **Launch Jupyter:**
    ```bash
    jupyter notebook
    ```

2.  **Training:** Open `fashionMNIST_stableDiffusion.ipynb` to execute the data loading, model definition, and training loop. This will save the trained VAE and UNet models to the `model/` directory.

3.  **Generation:** Open `fashionMNIST_stableDiffusionApplication.ipynb` to load the trained models and begin generating new, synthetic images of FashionMNIST items. You can experiment with different noise levels and sampling steps.

## üë• Team Members

This project was developed by:

*   SeifElden Osama
*   Sama NigmEldin
*   Habiba Ashraf
*   Mohamed Badr
*   Mohamed AbdAlwanis

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.
